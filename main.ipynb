import os
import time
!pip uninstall -q -y torch
!pip install -q --no-index --find-links=/kaggle/input/vllm-whl -U vllm
!pip install -q --no-index --find-links=/kaggle/input/grpc-and-ray-packages -U grpcio
!pip install -q --no-index --find-links=/kaggle/input/grpc-and-ray-packages -U "ray>=2.11" 
!pip install vllm
!pip install -U grpcio
!pip install -U "ray>=2.11"
!pip install pandas
from vllm import LLM, SamplingParams
import pandas as pd
from tqdm import tqdm
import gc
import sys
from collections import defaultdict
import torch
import re
import subprocess
MAX_MODEL_LEN = 4000
llm = LLM(model="/kaggle/input/deepseek-math" if ON_KAGGLE else "deepseek-ai/deepseek-math-7b-rl",
          dtype='half',
          enforce_eager=True,
          gpu_memory_utilization=0.99,
          swap_space=4,
          max_model_len=MAX_MODEL_LEN,
          kv_cache_dtype="auto" if ON_KAGGLE else 'fp8', # We found that using fp16 for the kv_cache improved the score
          tensor_parallel_size=2)
tokenizer = llm.get_tokenizer()
!mkdir codes
if PRIVATE:
    import aimo

    env = aimo.make_env()
    iter_test = env.iter_test()
    PROBLEMS_TO_SOLVE = 50
else:
    class DummyEnv:
        def __init__(self):
            self.results = []
        def predict(self, submission):
            self.results.append(submission['answer'])
        
    df = pd.read_csv('/kaggle/input/ai-mathematical-olympiad-prize/train.csv' if ON_KAGGLE else '/workspace/train.csv')
    iter_test = [({"problem":r[1]},{}) for _,r in df.iterrows()]
    env = DummyEnv()
    print("Running LOCALLY")
    PROBLEMS_TO_SOLVE = 10
%%writefile codes/runner.sh
#!/bin/bash

# Parallel code code execution in batches.
files_used=$1
batch_size=$2
file=0
for i in $(seq 0 $batch_size $files_used); do
    for j in $(seq 0 $batch_size); do
        if [ $((file+1)) -gt $files_used ]; then
            break
        fi
        timeout 4 python3 codes/code$file.py &> codes/result$file.txt &
        pids[${file}]=$!
        file=$((file+1))
    done
    # Wait for all processes in the current batch to finish
    for pid in ${pids[*]}; do
        wait $pid
    done
done
!chmod +x codes/runner.sh
code_wrap = """
import builtins

# Override the open function to prevent file operations
def disabled_file_operation(*args, **kwargs):
    raise PermissionError("File operations are disabled.")

builtins.open = disabled_file_operation

try:
    from sympy import *
    {}
except Exception as e:
    print('Fail', e)
    print('FAIL')
"""

def return_last_print(output, n):
    lines = output.strip().split('\n')
    if lines:
        return lines[n] # to do: bug
    else:
        return ""

def extract_integers(text):
    matches = re.findall(r'\d+', text)
    integers = [int(match) for match in matches]
    return integers

def preprocess_code(code):
    def repl(match):
        if "real" not in match.group():
            return "{}{}".format(match.group()[:-1], ', real=True)')
        else:
            return "{}{}".format(match.group()[:-1], ')')
    code = re.sub(r"symbols\([^)]+\)", repl, code)
    code = code.replace('\n', '\n    ')
    code = code_wrap.format(code)
    
    # Remove multiprocessing
    code = code.replace('multiprocessing', '')
    return code

def postprocess_code(output):
    CODE_STATUS = True
    try:
        return_value = return_last_print(output, -1)
    except Exception as e:
        return "No output", False
    print("CODE EVAL:", output)
    if return_value=='FAIL':
        CODE_STATUS = False
        try:
            return_value = return_last_print(output, -2)
        except:
            return_value = "No output"
        if "not defined" in return_value:
            return_value+='\nTry checking the formatting and imports'

    return return_value, CODE_STATUS  

def run_batch_of_codes(files_used):
    subprocess.run(['./codes/runner.sh', str(files_used), '4'])
    
    # Read results from files
    results = []
    for i in range(files_used):
        # try catch
        try:
            with open(f'codes/result{i}.txt', 'r') as f:
                result = f.read()
                result = postprocess_code(result)
                results.append(result)
        except Exception as e:
            results.append(("Error reading file", False))
            print(f"Error reading file {i}/{files_used-1}: {e}")
    return results

def process_text_output(output):
    result = output    
    try:
        result_output = re.findall(r'\\boxed\{(\d+)\}', result)

        if not len(result_output):
            result_output = -1
        else:
            result_output = result_output[-1]

        if not len(result_output):
            result_output = -1
        
        else:
            result_output = round(float(result_output)) % 1000
    
    except Exception as e:
        result_output = -1
    
    return result_output
        
def extract_code(text):
    if '```python' in text:
        split_text = text.split('```python')[-1]
        if '```' in split_text:
            return split_text.split('```')[0]
        else:
            return None
    else:
        return None
    
def extract_numbers(text):
    numbers = re.findall(r"[-+]?\d*\.\d+|[-+]?\d+", text)
    return [float(num) if '.' in num else int(num) for num in numbers]

def extract_answer_from_code_result(text):
    if text == -1:
        return -1
    nums = extract_numbers(str(text))
    if len(nums) != 1:
        return -1
    
    return int(round(nums[0])) % 1000
cot_prompt = """Below is a math problem you are to solve (positive numerical answer!):
\"{}\"
Analyze this problem and think step by step to come to a solution with programs. After solving the problem, output the final numerical answer within \\boxed{}.\n\n"""

code_prompt = """
Below is a math problem you are to solve (positive numerical answer):
\"{}\"
To accomplish this, first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step. Be clear so even an idiot can follow your instructions, and remember, your final answer should be positive integer, not an algebraic expression!
Write the entire script covering all the steps (use comments and document it well) and print the result. After solving the problem, output the final numerical answer within \\boxed{}.

Approach:
"""

def get_prompt(prompt, problem, continuation=None):
    final_prompt = "User:" + prompt.format(problem,"{}")
    
    if continuation:
        final_prompt += continuation
    
    return final_prompt

prompt_options = [
                  lambda problem, continuation=None: get_prompt(cot_prompt, problem, continuation),
                  lambda problem, continuation=None: get_prompt(code_prompt, problem, continuation),
                 ]

prompt_counts = [3, 4, 0, 0] if PRIVATE else [3, 4, 0, 0] # These prompt ratios we found were the best from all we tried.

def get_prompts(problem, continuation = None):
    prompts = []
    for prompt_creator, times_to_sample in zip(prompt_options, prompt_counts):
        prompts.extend(
            [prompt_creator(problem, continuation)] * times_to_sample
        )
        
    return prompts
def free_memory():
    for _ in range(5):
        torch.cuda.empty_cache()
        gc.collect()
        time.sleep(0.2)
stop_words = [tokenizer.eos_token if tokenizer is not None and tokenizer.eos_token is not None else '</s>']
stop_words.append("```output") # We stop as soon as we find the model trying to predict the output.

start_code_tokens = tokenizer('```python')['input_ids'][1:]
end_code_tokens = tokenizer('```')['input_ids'][1:]
print(start_code_tokens, end_code_tokens)
def process_token(token_ids, logits):
    # We check if the start_code_tokens exist.
    start_code_found = False
    start_code_found_idx = -1
    end_code_found = False
    end_code_found_idx = -1
    for i in range(len(token_ids)-len(start_code_tokens)+1):
        if token_ids[i:i+len(start_code_tokens)] == start_code_tokens:
            start_code_found = True
            start_code_found_idx = i
    
    if not start_code_found:
        return logits
    
    for i in range(start_code_found_idx + 1, len(token_ids) - len(end_code_tokens)+1):
        if token_ids[i:i+len(end_code_tokens)] == end_code_tokens:
            end_code_found = True
            end_code_found_idx = i
            break;
    
    # We force the model to output the code output tokens when it has finished to write the code.
    # This avoids situations when the model would just not get the output of the executed code.
    if end_code_found:
        if end_code_found_idx == len(token_ids)-1: 
            logits[185] = 10000
        # if newline we add the ```
        elif token_ids[-1] == 185:
            # we set the ``` token
            logits[10897] = 10000
        else:
            # "output" token
            logits[8157] = 10000
    else:
        logits = logits
        
        
    return logits
from dataclasses import dataclass

@dataclass
class SolutionDetails:
    prompt: str
    cummulative_code: str = ""
    last_executed_code_result: str = -1
    error_count: int = 0
    forced_output: bool = False
Time management
Time management based on recorded times, proved to be very reliable.

# Time management

SOFT_TIME_LIMIT = 60 * 60 * 8.2 if PRIVATE else 60 * 60 * 1# 8 hours
HARD_TIME_LIMIT = 60 * 60 * 8.4 if PRIVATE else 60 * 60 * 2# 8 and a half hours

class TimeManagement:
    
    def __init__(self, prompt_counts):
        # Initial counts, base time
        self.recorded_times = {
            (3, 4, 0, 0): 76, 
            (7, 10, 0, 0): 90.11047387123108, 
            (10, 14, 0, 0): 158.42619514465332, 
            (11, 15, 0, 0): 163.69477319717407, 
            (12, 17, 0, 0): 162.39567685127258, 
            (14, 19, 0, 0): 148.9604458808899, 
            (16, 23, 0, 0): 219.65354228019714, 
            (15, 20, 0, 0): 112.13560438156128, 
            (17, 24, 0, 0): 131.51007294654846, 
            (25, 35, 0, 0): 196.37973880767822, 
            (36, 50, 0, 0): 250.18024468421936, 
            (3, 5, 0, 0): 61.941444873809814, 
            (81, 108, 0, 0): 1550.27548289299, 
            (59, 78, 0, 0): 603.0817701816559, 
            (69, 93, 0, 0): 629.2428097724915, 
            (75, 100, 0, 0): 622.1524519920349, 
            (77, 103, 0, 0): 550.9045436382294, 
            (79, 105, 0, 0): 601.617894411087, 
            (89, 118, 0, 0): 517.1569063663483, 
            (149, 199, 0, 0): 2357.601425409317, 
            (142, 189, 0, 0): 1592.241544008255}
        self.time_start = None
        self.initial_prompt_counts = prompt_counts
    
    def _get_estimated_time(self, prompt_counts):
        # Calculate estimated time based on closest historical data or base time
        prompt_counts_tuple = tuple(prompt_counts)
        if prompt_counts_tuple in self.recorded_times:
            return self.recorded_times[prompt_counts_tuple]
        
        closest_key = min(self.recorded_times.keys(), key=lambda k: self._normalized_distance(k, prompt_counts_tuple))
        base_time = self.recorded_times[closest_key]
        # Adjust time by scalar of difference in sum of counts
        sum_initial = sum(closest_key)
        sum_current = sum(prompt_counts_tuple)
        return base_time + (sum_current - sum_initial) * 8  # Adding or subtracting time based on counts change
    
    def _normalized_distance(self, key1, key2):
        # Calculate Euclidean distance between two tuples
        max_length = max(len(key1), len(key2))
        extended_key1 = key1 + (0,) * (max_length - len(key1))
        extended_key2 = key2 + (0,) * (max_length - len(key2))
        return sum((x - y) ** 2 for x, y in zip(extended_key1, extended_key2)) ** 0.5
    
    def record_start(self):
        self.time_start = time.time()
        
    def record_end(self, prompt_counts):
        if self.time_start is None:
            return
        prompt_counts_tuple = tuple(prompt_counts)
        elapsed_time = time.time() - self.time_start
        if prompt_counts_tuple in self.recorded_times:
            self.recorded_times[prompt_counts_tuple] = 0.3*elapsed_time + 0.7*self.recorded_times[prompt_counts_tuple]
        else:
            self.recorded_times[prompt_counts_tuple] = elapsed_time
        self.time_start = None
    
    def exit_early(self, prompt_counts) -> bool:
        global NOTEBOOK_START_TIME, HARD_TIME_LIMIT
        time_left = HARD_TIME_LIMIT - (time.time() - NOTEBOOK_START_TIME)
        estimated_time = self._get_estimated_time(prompt_counts)
        return time_left < estimated_time
    
    def maximize_prompt_counts(self, prompt_counts, problems_left):
        global NOTEBOOK_START_TIME, SOFT_TIME_LIMIT, HARD_TIME_LIMIT
        time_left = (SOFT_TIME_LIMIT if problems_left != 1 else HARD_TIME_LIMIT) - (time.time() - NOTEBOOK_START_TIME)

        last_scale_factor = None
        scale_factor = 0.1
        
        while scale_factor < 50.0:
            scaled_counts = [int(x * scale_factor) for x in self.initial_prompt_counts]
            estimated_time = problems_left * self._get_estimated_time(scaled_counts)
            
            if estimated_time > time_left:
                break;
            last_scale_factor = scale_factor
            
            scale_factor += 0.01
            
        if last_scale_factor == None:
            return prompt_counts
        

        updated_prompt_counts = [int(x * last_scale_factor) for x in self.initial_prompt_counts]
        if sum(updated_prompt_counts) == 0:
            return prompt_counts

        return updated_prompt_counts


    
time_management = TimeManagement(prompt_counts)
@dataclass
class LLMResult:
    prompt: str
    completion: str
Main prediction loop
MAX_NUM_ITERATIONS = 7

# Per iteration we allow a max of 2k tokens
sampling_params = SamplingParams(temperature=0.9,
                                 max_tokens=2000,
                                 top_p=1.0,
                                 logits_processors=[process_token],
                                 stop=stop_words)
problems_solved = 0
for test, sample_submission in iter_test:
    
    time_management.record_start()
    prompt_counts = time_management.maximize_prompt_counts(prompt_counts, PROBLEMS_TO_SOLVE - problems_solved)
    print("New prompt counts:", prompt_counts)
    
    problem = test['problem'].iloc[0] if PRIVATE else test['problem']
    prompts_and_code = [SolutionDetails(prompt=p) for p in get_prompts(problem)]
    
    times_iterated = 0
    solutions = []
    _solutions = defaultdict(list)
    
    # initialize an array with size len(prompts_and_code) 
    while len(prompts_and_code) > 0 and times_iterated < MAX_NUM_ITERATIONS:
        # Generate all outputs
        prompts = [pc.prompt for pc in prompts_and_code]
        responses = llm.generate(prompts, sampling_params)
        responses = [LLMResult(response.prompt, out.text) for response in responses for out in response.outputs]

        # write all code responses to files
        prompt_to_code = [-1] * len(prompts_and_code)

        files_needed = 0
        for i, (pc, response) in enumerate(zip(prompts_and_code, responses)):
            response = response.completion
            extracted_code = extract_code(response)
            if extracted_code:
                code = preprocess_code(pc.cummulative_code + "\n" + extracted_code)
                prompt_to_code[i] = files_needed
                with open(f'codes/code{files_needed}.py', 'w') as f:
                        f.write(code)
                files_needed += 1

        outputs = run_batch_of_codes(files_needed)

        prompts_and_code_new = []
        
        
        for i, (pc, candidate) in enumerate(zip(prompts_and_code, responses)):
            
            cummulative_code = pc.cummulative_code
            last_executed_code_result = pc.last_executed_code_result
            code_error_count = pc.error_count
            
            print("================================")
            text = candidate.completion
            prompt = candidate.prompt
            
            if prompt_to_code[i] != -1:
                executed_code, CODE_STATUS = outputs[prompt_to_code[i]]
                if executed_code != -1:
                    text += "\n```output\n"+executed_code+"\n```\n"
                    if CODE_STATUS:
                        extracted_code = extract_code(text)
                        cummulative_code += extracted_code
                else:
                    cummulative_code = ""
                        
                new_prompt = prompt + text
                if last_executed_code_result == executed_code:
                    code_error_count+=1
                else:
                    code_error_count = 0
                                                
                last_executed_code_result = executed_code

                if len(tokenizer(new_prompt)['input_ids']) < MAX_MODEL_LEN and not pc.forced_output:
                        
                    if code_error_count >= 1:
                        print("REPEATED ERRORS OR RESULT, PRUNING")
                        continue;
                            
                    prompts_and_code_new.append(
                        SolutionDetails(prompt=new_prompt, 
                                        cummulative_code=cummulative_code, 
                                        last_executed_code_result=last_executed_code_result, 
                                        error_count=code_error_count)
                    )
                        
                else:
                    last_executed_code_result = extract_answer_from_code_result(last_executed_code_result)
            else:
                # If not we are already done
                output = process_text_output(prompt + text)
                if output != -1:
                    solutions.append([output, prompt+text])
                    _solutions[output].append(prompt+text)
                elif not '\\boxed{' in text and not pc.forced_output:
                    print("Forcing output on next turn.")
                    
                    # We force the model to output the result in boxed by appending the line below.
                    prompts_and_code_new.append(
                        SolutionDetails(prompt=prompt + text + "\nThe final answer is \\boxed{", 
                                        cummulative_code=cummulative_code, 
                                        last_executed_code_result=-1, 
                                        error_count=code_error_count,
                                        forced_output=True)
                    )
                last_executed_code_result = extract_answer_from_code_result(last_executed_code_result)
        
        prompts_and_code = prompts_and_code_new
        times_iterated += 1
        
        # Early exitting
        number_of_votes = sum([len(_solutions[k]) for k in _solutions])
        threshold = 10
        
        if len(_solutions) < 3 and number_of_votes < threshold:
            continue;
            
        sorted_occurrences = list(sorted([len(_solutions[k]) for k in _solutions]))
        if number_of_votes >= threshold:
            if sorted_occurrences[-1] > number_of_votes/2:
                print("EARLY EXIT")
                break;
        
        
        maximum_new_votes = len(prompts_and_code_new)
        diff_to_cover = sorted_occurrences[-1]-sorted_occurrences[-2]
        if diff_to_cover > maximum_new_votes:
            print("EARLY EXIT")
            break;
            
        if number_of_votes > 15 and len(_solutions) < 3:
            print('early exit')
            break;

                                                                            
    counts = defaultdict(int)
        
    for i in solutions:
        try:
            i[1] = i[1].lower()
            i[0] = process_text_output(i[1])
            
            # We only give a small number of points for purely text based answers.
            counts[i[0]]+=0.05
            code_outputs = [j.split('\n```')[0] for j in i[1].split('```output\n')[1:]]
            
            # We instead give most of it if the code agrees with the text output.
            for code_output in code_outputs:
                
                # We check if it is purely code, if not we extract and 
                # give points for each number which is equal to the text one.
                try:
                    if i[0] == int(code_output):
                        counts[i[0]]+=0.8
                except:
                    nums = extract_numbers(code_output)
                    for n in nums:
                        if i[0] == n:
                            counts[n]+=1.0 # 1.0 here was the best on validation but it should probably be the same as the one above.
        except:
            pass

    counts[-1] = -123123
    
    # We found that penalizing numbers that are small or that are part of the problem statement
    # improved the score both on validation and public leaderboard.
    for i in range(0,11):
        counts[i]-= 0.05 * (prompt_counts[0] + prompt_counts[1])
        
    for num in extract_numbers(problem):
        counts[num]-=0.10 * (prompt_counts[0] + prompt_counts[1])
    print(counts)

    
    # Select the best result
    best_solution = max(counts, key=counts.get)
    sample_submission['answer'] = best_solution
    env.predict(sample_submission)
    print("FINAL ANSWER: ", best_solution)
    print ("==========================================================================================")
    
    time_management.record_end(prompt_counts)
    
    problems_solved += 1
!rm -rf codes
problems_solved
if not PRIVATE:
    print(env.results)
print(time_management.recorded_times)
